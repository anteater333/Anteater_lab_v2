---
layout: post
title: '프로젝트 회고록 보관소 #1 숲Soup - 나무위키 인기 검색어 크롤러'
date: 2022-05-12 10:13:00 +0900
categories: [Project]
---

## 숲Soup - 나무위키 인기 검색어 크롤러

> "바쁜 현대사회, 가끔 나무 대신 숲을 봐야 할 때도 있습니다."

![첫 화면](https://i.postimg.cc/qv9CQH86/soups1.png)

![둘 화면](https://i.postimg.cc/85bJ1FxL/soups2.png)

![기능](https://i.postimg.cc/wTrtTY44/soups3.png)

현재 구조도
![구조도](작성예정)

개선하고싶은 구조도

이하 노션 발췌

------

# 숲Soup

나무위키 실검 크롤링 + 각 순위에 대한 시한부 메모

## 의미

나무를 보지 않고 숲을 본다 + 크롤링 라이브러리에 붙는 soup

## 서비스 구조 및 아이디어

웹페이지로 제공

단일 구조 서비스가 적합할 것으로 예상

사용자가 웹페이지에 접속할 때 크롤링을 하는 것이 아니라 서버에서 일정 시간 마다 크롤링해서 스냅샷을 가지고 있도록 하기

~~realtime db가 필요할 듯~~ 폐기

짧은 생명주기를 가지는 메모 기능

다양한 사람이 편집할 수 있어야함

시범적인 기능의 느낌

## 메모에 대한 자료구조

일단 업로드된 메모는 모두가 수정/삭제 가능

경쟁적 수정 허용

일종의 세마포어 지원, 누군가 수정 중인 상태에서 이 메모 세션에 수정 완료가 커밋되면 그 세션에 대한 모든 세마포어 폐기 ⇒ 새로운 세마포어 생성

## 개발 도구

react + bootstrap WAS

파이썬 or Node 크롤러

DB는 따로 필요 없음 데이터는 모두 런타임에만 존재해도 문제 없음

# 프로젝트 회고

개발 과정까지만 해도 아주 낭만적이었어요... 배포 과정에서 크게 혼났다.

## 개발 과정

Back

- 마냥 soup 시리즈 하면 될 줄 알았는데, selenium이라는 강력한 라이브러리+툴체인을 사용하였다.
- 덕분에 개발은 편하였으나... 어찌보면 배포과정에 일어난 대부분의 문제가 selenium 관련...
- 그 외에 express로 간단하게 API 만들었고, 하면서 이것저것 추가하고싶어진다는 유혹을 받긴 했는데 그럭저럭 물리침

Front

- React, 과정의 역전이 발생한 것 같은데, React를 배우고 RN을 하는게 아니라 RN을 하고 React를 써버리기.
- 마냥 구상하는 중엔 그냥 서버 하나 두고 걔가 크롤링도 하고 웹앱도 쏴주면 되는거 아니야? 라고 생각했는데, 웹 어플리케이션의 근본적 구조에 대한 이해가 부족했음을 알게 되었음.
    - SSR의 가치에 대해서도 배웠다.
    - 짧게 요약하자면, 웹앱의 코드는 클라이언트에게 그대로 전해진다.
        
        ⇒ 웹앱에서 fs를 사용하면 클라이언트의 파일 시스템에 접근하게 된다는 보안적 이슈.
        
- 그 외엔, 개발 과정의 막바지에 달해서 완전 정신이 몽롱해지니까 코드가 난장판이 됐다는 점 정도...
    
    

## 배포 과정

이미지 생성

- Docker를 사용한다는 아이디어
- 개발자 커뮤니티에 감탄을 한 것이, Selenium Docker Image가 있다는 사실에 추호의 의심도 필요없었다. 손쉽게 찾았음.
- 이미지 빌드를 위한 학습 과정에서 dockerfile과 docker-compose의 차이와 각 필요성을 파악
- Selenium + git 설치 + node 설치 + npm install로 빌드순서 결정
- (일단은 완벽하진 않지만) 이미지를 만들었다고 파악한 다음 이것을 배포할 환경을 물색

배포

- 처음 생각난 건, AWS
    - 근데 무료티어 끝남...
- 대체재를 찾는다고 시간을 많이 썼다.
    - 일단 평생 무료 인스턴스는 애초에 잘 없다는 것을 배웠음
- 그렇게 떠올린 아이디어
    - 도커 이미지도 (실제론 엉성하지만) 깔쌈하게 만들어냈는데, 그냥 이거 이미지 가지고 만든 컨테이너를 배포하게 도와주는 서비스는 없나?
    - Heorku가 적합하다고 파악
- 그 과정에서... Dockerfile 문제점 발견
    - 마지막에 CMD를 넣어서 npm start를 실행했다
    - 사실 selenium docker image는 selenium을 실행시키는 스크립트를 최초로 실행해줘야 했다
    - 하지만 dockerfile에서 cmd는 하나만 인정받기 때문에 내가 만든 이미지에서 명령어가 덮어쓰이게 된 것
    - 해당 스크립트의 위치를 알아 내었고(깃허브 뒤져봄), sh 스크립트 파일을 작성해서 순차적으로 명령어가 여러개 실행되도록 만들었다.
- Docker는 일단락 되었다. 문제는 스펙
- 일단 헤로쿠는 결론적으로 실패했다.
    - 빌드까진 어찌저찌 했는데, 서버가 실행되더니 오류를 뱉고 사망
    - 메모리 문제인 것으로 파악햇다. 헤로쿠의 무료 dyno는 512MB 메모리를 가진다.
    - selenium 이미지는 본인들 깃허브 readme 문서에서 컨테이너 실행 시 2gb 메모리를 할당하도록 권장하더라
- 울며 겨자먹기로 인스턴스를 제공해주는 서비스를 찾으러 다시 돌아갔다.
- 네이버 클라우드 플랫폼은 사실 헤로쿠 전에 시도했는데, 똑같이 메모리 문제를 겪었다.
- Azure, AWS의 대항마. 계정을 만들고 크레딧을 받았다.
- 인스턴스를 만들었고, 몇가지 귀찮은 과정을 거친 다음에 Docker 이미지를 올렸다.
- 하지만 안되잖아!
- 처음엔 문제가 service 어플리케이션 코드에서 selenium 주소를 지정하지 않아서 selenium을 못찾는 문제라고 생각했는데⇒ 설명이 장황하지만 어쨌든 이거 문제는 아니었음
- 자꾸 엘레멘트를 못가져오겠대
- 그래서 가장 쉬운 방법, console.log를 찍어봤다. 대체 뭘 가져오는거야 이놈이
- 가져온 페이지는 바로 ... cloudflare... 로봇 테스트....
- 처참한 결과물에 결국 배포 포기 선언
- 원대한 꿈이... 스러져가는구나...

사실 뭐 그정도로 대단한 프로젝트는 아니고, 플랜 C 정도로 생각했던건, 가정용 PC를 서버로 못만드나... 하는 거였는데, 예 뭐. 전기세는 누가 냅니까.

나중에 블로그에 잘 정리해서 회고 글이나 써보자.

------

220412

별로 적절하지 않은 비유가 떠올랐으나 바로 마음 속 깊은 곳으로 집어넣어버렸고,

아무튼 자꾸 이 프로젝트가 눈에 아른거려가지고 오늘도 충동적으로 예토전생 시도해버렸다.

결론부터 말하자면, 실패했다.

그럼에도 불구하고 지금 살짝 들떠있을 수 있는 이유는, 아무 수확이 없었던 것도 아니기 때문.

1. Selenium은 애초에 필요하지 않았다.
    
    내가 처음 결정지었던 방법은 Selenium을 통해 검색창에 이벤트를 발생시키고, 그 이벤트의 결과로 생성된 검색어 리스트를 가져오는 방식
    
    그 짓거리를 하면서도 느꼈지만 마치 심하게 흔들리는 외줄을 타는 것과 같은 느낌이었다.
    
    생각해보면 당연히 떠올려야 되는 건데, 나무위키 프론트엔드 웹 앱이 그 이벤트 발생 했을 때 실검순위 주세요 하고 요청을 보내는 주소가 있을거 아니야. 그래서 브라우저의 개발자 도구를 통해 그 주소를 찾아냈다.([http://search.namu.wiki/api/ranking](https://search.namu.wiki/api/ranking))
    
    그걸 가저오기 위해 크롤러에서 거추장스러웠던 셀레니움을 들어냈고, ajax 라이브러리들을 시도해 보았다.
    
2. cURL은 엄청나게 강력한 도구다.
    
    라이브러리들은 저 url에서 데이터를 가져오는데 실패하였다. 하지만 쉘에서 진행한 curl 테스트는 항상 성공하였다. 어떤 비밀이 있는건지... 심지어 curl을 그대로 가져다 쓸 것 만 같은 이름을 가진 node-libcurl 라이브러리도 안통하더라.
    
    그래서 떠올린 방법, 작년에 지겹도록 만진 Child_process를 쓰자. 그 말인 즉슨, curl을 그대로 가져다가 쓰자. 완벽하게 동작하더라.
    
3. 그래도 여전히 안된다.
    
    결국 IP가 블랙리스트에 올라가있으면 안된다. 대부분의 클라우드 서비스는 이미 블랙리스트에 올라가 있는 것으로 보인다. 절망...
    
4. 희망회로
    
    이걸 해결할 수 있는 방법은 정녕 없는 것인가? 내 뇌내 망상세계에서 한 가지 설계도가 그려졌다. 다만, 홈 서버를 성공적으로 구축한다는 행복한 시나리오가 필요하다.
    
    FE - BE - HOMESERVER - NAMU.WIKI
    
    홈서버는 비정기적으로 나무위키에서 검색어 순위를 가져온다.
    
    백엔드는 홈서버로부터 검색어 순위를 가져온다.
    
    그렇다, 단계를 하나 더 줬을 뿐이다.
    
    홈서버는 마치 로드 밸런서처럼 동작한다. 가치를 고평가하자면, 나는 요청이 많을 곳과 요청이 아주 적을 곳으로 BE를 분리했다고 볼 수 있다.
    
    홈 서버는 그냥 가정용 컴퓨터고, 최대한 일반 사람 사용자가 나무위키를 적당히 자주 들어가보는 것 처럼 행동해야 한다.(블랙리스트에 오르면 나도 피곤해져)
    
    10분 정도에 한 번 씩 순위를 파싱, 새벽처럼 사용자들이 많이 없을 시간대에는 1시간, 2시간 이렇게 텀을 길게 주기도 하고. 아니면 12~8시 동안엔 슬립 모드에 들어가도록 한다거나.
    
    그리고 백엔드 서버로부터만 외부 접근이 가능하도록 방화벽을 설정한다.
    
    백엔드 서버는 주기적으로(아마 홈 서버의 최단 크롤링 간격 시간 만큼, 혹은 둘 사이에 소켓을 둬, 소켓을 두는게 더 좋겠다) 홈 서버의 정보를 읽어서 저장해두고, 홈 서버의 상태를 나타내는 메세지 처리도 철저하게 해둬야겠지. → 프론트도 함께 연계를 해야하고.
    
    백엔드 서버의 지금 코드 구조 또한 크게 바뀔 필요 없을 수도 있다. 홈 서버가 나무위키인 것 처럼 행동하면 될 테니까. 
    
    그래서...
    
    PROFIT!! - 진짜 옛날 드립인데 이것도.